# Word Index Building - Critical Fixes and Performance Improvements

## Summary

This document describes the critical bug fixes and performance improvements made to the `BuildWordIndexAsync` method in `SqlIndexRepository.cs`. The changes address both **correctness issues** that could cause data loss/corruption and **performance issues** that caused the indexing process to take days.

---

## Changes Made

### 1. **CRITICAL FIX: Case-Insensitive Lemma Matching** ‚úÖ

**Issue**: Word and lemma assignment to spans failed when lemma had different case.

**Location**:
- `AssignWordIdsAsync` (line ~1238)
- `InsertLemmataAsync` (lines ~1548, ~1561)

**Problem**:
- The `word` and `lemma` tables store values as `LOWER(lemma)` and `LOWER(value)`
- But the UPDATE statements used case-sensitive comparisons: `span.lemma = word.lemma`
- This meant spans with "Lemma" would not match words/lemmata with "lemma"

**Fix**:
```sql
-- BEFORE (incorrect)
AND span.lemma = word.lemma

-- AFTER (correct)
AND LOWER(span.lemma) = word.lemma
```

**Impact**:
- **HIGH** - Some spans were not getting `word_id` and `lemma_id` assigned
- This would cause incorrect or missing word counts
- All three FK assignment locations fixed

---

### 2. **CRITICAL FIX: Complete Rewrite of Word Counts Calculation** üöÄ

**Issue**: The original implementation had multiple critical flaws:

#### 2a. Race Condition in Batch Flushing
**Location**: Original `InsertWordCountsAsyncFor` (line ~1920)

**Problem**:
```csharp
// Multiple threads checking and clearing a ConcurrentBag
if (counts.Count == batchSize)  // ‚ùå Race condition!
{
    await BatchInsertWordCounts(connection2, [.. counts]);
    counts.Clear();  // ‚ùå Other threads may have just added items!
}
```

- Thread A checks `count == 1000`, enters the if block
- Thread B adds item #1001 before Thread A calls Clear()
- Thread A clears the bag, losing Thread B's item

#### 2b. Batch Flush Logic Error
**Problem**: Batch flush only happened at end of all pairs for ONE word, not after each word.

- For word #1, counts were added to the bag for ALL pairs
- Batch flush only happened if bag was full or after ALL words processed
- This meant most counts were never inserted!

#### 2c. Severe Performance Issue
**Problem**: The algorithm executed **one SQL query per word per document pair**:
- 100,000 unique words √ó 500 document pairs = **50 million SQL queries**
- Each query required a network roundtrip to PostgreSQL
- This took **days** to complete

**Solution**: Complete rewrite using **bulk SQL with UNION ALL**

**New Approach**:
1. Build a single massive SQL query with all pairs as UNION ALL clauses
2. Each UNION clause counts words for one document attribute pair
3. Execute once, inserting all counts in a single operation

**Code**:
```csharp
private async Task InsertWordCountsAsync(IDbConnection connection,
    IList<DocumentPair> docPairs, CancellationToken cancel,
    IProgress<ProgressReport>? progress = null)
{
    StringBuilder sql = new();
    sql.Append("INSERT INTO word_count(word_id, lemma_id, doc_attr_name, doc_attr_value, count)\n");

    for (int i = 0; i < docPairs.Count; i++)
    {
        if (i > 0) sql.Append("\nUNION ALL\n");
        AppendWordCountUnionClause(docPairs[i], sql);
    }

    DbCommand cmd = (DbCommand)connection.CreateCommand();
    cmd.CommandText = sql.ToString();
    cmd.CommandTimeout = 3600; // 1 hour for potentially long operation
    await cmd.ExecuteNonQueryAsync(cancel);
}
```

**Helper Method**:
```csharp
private void AppendWordCountUnionClause(DocumentPair pair, StringBuilder sql)
{
    sql.Append("SELECT s.word_id, s.lemma_id, ");
    sql.Append($"'{SqlHelper.SqlEncode(pair.Name)}' AS doc_attr_name, ");

    if (pair.IsNumeric)
        sql.AppendFormat("'{0:F2}:{1:F2}' AS doc_attr_value, ", pair.MinValue, pair.MaxValue);
    else
        sql.Append($"'{SqlHelper.SqlEncode(pair.Value!)}' AS doc_attr_value, ");

    sql.Append("COUNT(*) AS count\n");

    if (pair.IsPrivileged)
    {
        sql.Append("FROM span s INNER JOIN document d ON s.document_id = d.id\n");
        sql.Append("WHERE s.word_id IS NOT NULL AND ");
        AppendDocPairClause("d", pair, sql);
    }
    else
    {
        sql.Append("FROM span s INNER JOIN document_attribute da ON s.document_id = da.document_id\n");
        sql.Append("WHERE s.word_id IS NOT NULL AND ");
        AppendDocAttrPairClause("da", pair, sql);
    }

    sql.Append("\nGROUP BY s.word_id, s.lemma_id");
}
```

**Performance Gain**:
- **FROM**: 50,000,000 queries over days
- **TO**: 1 query in minutes/hours
- **Estimated improvement: 100-1000x faster** ‚ö°

**Correctness**:
- No race conditions (single-threaded SQL execution)
- No lost data (all counts in one atomic operation)
- Proper SQL encoding throughout

---

### 3. **Security Fix: SQL Injection Prevention**

**Location**: `AppendDocPairClause` (line ~1785)

**Problem**:
```csharp
// BEFORE
sql.Append($"{table}.{pair.Name}='{pair.Value}'");
```

While `pair.Value` comes from the database, it's best practice to encode it.

**Fix**:
```csharp
// AFTER
sql.Append($"{table}.{pair.Name}='{SqlHelper.SqlEncode(pair.Value!)}'");
```

**Impact**: LOW - Defensive programming, prevents potential issues with special characters.

---

## Algorithm Correctness Verification

### Overall Flow ‚úÖ
1. **Clear index** ‚Üí Clear all existing word/lemma data
2. **Insert words** ‚Üí Group tokens by (language, value, pos, lemma), apply exclusions
3. **Assign word_id to spans** ‚Üí Using same exclusions and case-insensitive matching
4. **Insert lemmata** ‚Üí Group words by (language, pos, lemma), sum counts
5. **Assign lemma_id to words and spans** ‚Üí Case-insensitive matching, only spans with word_id
6. **Build document pairs** ‚Üí Collect all attr name=value combinations with numeric binning
7. **Calculate word counts** ‚Üí Count words per document pair (NOW: bulk SQL)
8. **Calculate lemma counts** ‚Üí Sum word counts grouped by lemma (already bulk SQL)

### Data Integrity ‚úÖ

**Word Building**:
- ‚úÖ Correctly groups by language, value, pos, lemma
- ‚úÖ Uses `LOWER()` for value and lemma
- ‚úÖ Applies exclusions (POS, span attributes)
- ‚úÖ FK assignment uses same criteria with LOWER()

**Lemma Building**:
- ‚úÖ Correctly groups by language, pos, lemma
- ‚úÖ Sums word counts
- ‚úÖ FK assignment to both word and span tables
- ‚úÖ Span update restricted to spans with word_id (preserves exclusions)
- ‚úÖ Uses LOWER() for matching

**Document Pairs**:
- ‚úÖ Separates privileged vs non-privileged attributes
- ‚úÖ Numeric binning logic correct
- ‚úÖ Exclusions applied correctly

**Word Counts** (NEW):
- ‚úÖ Counts all words for all pairs in single query
- ‚úÖ Properly distinguishes privileged vs non-privileged attributes
- ‚úÖ Numeric ranges and string values handled correctly
- ‚úÖ All values properly SQL-encoded
- ‚úÖ Only counts spans with word_id IS NOT NULL

**Lemma Counts**:
- ‚úÖ Simple SUM aggregation from word_counts
- ‚úÖ Already correct in original implementation

---

## Expected Results

### Performance
| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Word count queries | 50M | 1 | 50,000,000x fewer |
| Network roundtrips | 50M | 1 | 50,000,000x fewer |
| Estimated time (1.5M tokens) | Days | Minutes-Hours | 100-1000x faster |
| CPU usage | 20% | Higher | Better utilization |
| Memory usage | Low | Higher (query string) | Acceptable trade-off |

### Data Quality
- **Before**: Some spans missing word_id/lemma_id due to case mismatch
- **After**: All eligible spans correctly assigned
- **Before**: Word counts potentially incomplete/corrupted due to race conditions
- **After**: All word counts accurate and complete

---

## Testing Recommendations

### 1. Small Dataset Test
Run on the example dataset (Catullus + Horatius):
```bash
./pythia index-w -d pythia-demo -c date-value=3 -c date_value=3 -x date
```

**Verify**:
- All words have counts matching span counts
- All lemmata counts equal sum of their word counts
- Query from docs/14-example-counts.md returns expected results

### 2. Performance Test
On your 1.5M token, 350 document corpus:
- **Monitor**: Execution time (should be hours, not days)
- **Monitor**: CPU usage (should be higher than 20%)
- **Monitor**: Memory usage (SQL query string may be large)
- **Monitor**: PostgreSQL query log (should see one massive INSERT)

### 3. Data Integrity Test
```sql
-- 1. Check all tokens have word_id (except excluded ones)
SELECT COUNT(*) FROM span
WHERE type='tok' AND lemma IS NOT NULL
AND word_id IS NULL
-- Should be 0 (or only excluded spans)

-- 2. Verify word counts match span counts
SELECT w.id, w.value, w.count, COUNT(s.id) as span_count
FROM word w
LEFT JOIN span s ON s.word_id = w.id
GROUP BY w.id, w.value, w.count
HAVING w.count != COUNT(s.id);
-- Should return 0 rows

-- 3. Verify lemma counts match word counts
SELECT l.id, l.value, l.count, SUM(w.count) as word_count_sum
FROM lemma l
LEFT JOIN word w ON w.lemma_id = l.id
GROUP BY l.id, l.value, l.count
HAVING l.count != SUM(w.count);
-- Should return 0 rows

-- 4. Check word_count table completeness
-- Each word should have entries for each doc pair
SELECT w.id, COUNT(DISTINCT wc.doc_attr_name) as attr_count
FROM word w
LEFT JOIN word_count wc ON wc.word_id = w.id
GROUP BY w.id
ORDER BY attr_count;
-- All should have similar counts (# of unique attr names)
```

---

## Rollback Plan

If issues arise, revert these changes:
1. Restore `SqlIndexRepository.cs` from previous commit
2. The original algorithm was slower but had fewer simultaneous issues
3. However, the case-sensitivity bug existed in original - need to apply that fix separately

**Recommended**: Test on a copy of the database first!

---

## Future Optimization Opportunities

If further performance is needed:

### 1. Batch the UNION ALL Query
If you have 1000s of document pairs, the SQL query string may become too large.
**Solution**: Break into batches of 100 pairs each.

### 2. Parallel Batch Execution
Execute multiple batched queries in parallel (but fewer than current approach).

### 3. Temporary Tables
Create temp tables with pre-joined data, add indexes, then query.

### 4. Tune PostgreSQL
- Increase `work_mem` for large GROUP BY operations
- Adjust `shared_buffers` for better caching
- Use `EXPLAIN ANALYZE` on the generated query

### 5. Consider Partitioning
For very large corpora, partition the `span` table by document_id.

---

## Changed Files

- ‚úÖ `Pythia.Sql/SqlIndexRepository.cs`
  - Fixed `AssignWordIdsAsync` (case-insensitive matching)
  - Fixed `InsertLemmataAsync` (case-insensitive matching, both updates)
  - Rewrote `InsertWordCountsAsync` (bulk SQL approach)
  - Added `AppendWordCountUnionClause` (helper for bulk SQL)
  - Fixed `AppendDocPairClause` (SQL encoding)
  - Removed `InsertWordCountsAsyncFor` (replaced by bulk approach)
  - Note: `GetWordLemmaIdsAsync` and `GetMax` now unused (safe to remove later)

---

## Conclusion

These changes address **critical correctness bugs** that would have caused incomplete/incorrect data, and provide a **massive performance improvement** (100-1000x faster) by replacing millions of individual queries with a single bulk operation.

The new implementation is:
- ‚úÖ **Correct**: No race conditions, no lost data
- ‚úÖ **Complete**: All counts calculated atomically
- ‚úÖ **Fast**: Single query instead of millions
- ‚úÖ **Safe**: Proper SQL encoding throughout
- ‚úÖ **Maintainable**: Simpler logic, easier to understand

**Recommendation**: Test thoroughly on a copy of your production database before deploying to production.
